{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from IPython.core.display import display, HTML\n",
    "# display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv, re, string, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "remove_after = r' as in | refers to | etc.| also written | also known | e\\.g\\.|; |–|—| that is | that is,'\n",
    "split_by = r'<br\\/>| \\/ |\\n| \\\\ | or '\n",
    "acr_punc =  '!\"\\'(),.:;<=>?`#%'\n",
    "note_punc = '!\"\\'(),.:;<=>?`#%()[]'\n",
    "peroid = r'\\.\\s+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rishubj/.local/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2818: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ROW_ID  SUBJECT_ID   HADM_ID   CHARTDATE CHARTTIME STORETIME  \\\n",
      "0     174       22532  167853.0  2151-08-04       NaN       NaN   \n",
      "1     175       13702  107527.0  2118-06-14       NaN       NaN   \n",
      "2     176       13702  167118.0  2119-05-25       NaN       NaN   \n",
      "3     177       13702  196489.0  2124-08-18       NaN       NaN   \n",
      "4     178       26880  135453.0  2162-03-25       NaN       NaN   \n",
      "\n",
      "            CATEGORY DESCRIPTION  CGID  ISERROR  \\\n",
      "0  Discharge summary      Report   NaN      NaN   \n",
      "1  Discharge summary      Report   NaN      NaN   \n",
      "2  Discharge summary      Report   NaN      NaN   \n",
      "3  Discharge summary      Report   NaN      NaN   \n",
      "4  Discharge summary      Report   NaN      NaN   \n",
      "\n",
      "                                                TEXT  \n",
      "0  Admission Date:  [**2151-7-16**]       Dischar...  \n",
      "1  Admission Date:  [**2118-6-2**]       Discharg...  \n",
      "2  Admission Date:  [**2119-5-4**]              D...  \n",
      "3  Admission Date:  [**2124-7-21**]              ...  \n",
      "4  Admission Date:  [**2162-3-3**]              D...  \n"
     ]
    }
   ],
   "source": [
    "dfile='/home/rishubj/text/MIMIC-data/NOTEEVENTS.csv'\n",
    "chunksize = 10 ** 8\n",
    "for chunk in pd.read_csv(dfile, chunksize=chunksize):\n",
    "    print chunk.head()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk.to_csv('/home/rishubj/text/MIMIC-data/chunk0NOTEEVENTS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Admission Date:  [**2119-5-4**]              Discharge Date:   [**2119-5-25**]\\n\\n\\nService: CARDIOTHORACIC\\n\\nAllergies:\\nAmlodipine\\n\\nAttending:[**Last Name (NamePattern1) 1561**]\\nChief Complaint:\\n81 yo F smoker w/ COPD, severe TBM, s/p tracheobronchoplasty [**5-5**]\\ns/p perc trach [**5-13**]\\n\\nMajor Surgical or Invasive Procedure:\\nbronchoscopy 3/31,4/2,3,[**6-12**], [**5-17**], [**5-19**]\\ns/p trachealplasty [**5-5**]\\npercutaneous tracheostomy [**5-13**] after failed extubation\\ndown size trach on [**5-25**] to size 6 cuffless\\n\\n\\nHistory of Present Illness:\\nThis 81 year old woman has a history of COPD. Over the past five\\n\\nyears she has had progressive difficulties with her breathing.\\nIn\\n[**2118-6-4**] she was admitted to [**Hospital1 18**] for respiratory failure\\ndue\\nto a COPD exacerbation. Due to persistent hypoxemia, she\\nrequired\\nintubation and a eventual bronchoscopy on [**2118-6-9**] revealed marked\\n\\nnarrowing of the airways on expiration consistent with\\ntracheomalacia.\\nShe subsequently underwent placement of two\\nsilicone stents, one in the left main stem and one in the\\ntrachea. During the admission the patient had complaints of\\nchest\\npain and ruled out for an MI. She was subsequently discharged to\\n\\n[**Hospital1 **] for physical and pulmonary rehab. Repeat bronchoscopy\\non\\n[**2118-8-1**] revealed granulation tissue at the distal right lateral\\nwall of the tracheal stent. There was significant malacia of the\\n\\nperipheral and central airways with complete collapse of the\\nairways on coughing and forced expiration. Small nodules were\\nalso noted on the vocal cords. She has noticed improvement in\\nher\\nrespiratory status, but most recently has been in discussion\\nwith Dr. [**First Name4 (NamePattern1) 951**] [**Last Name (NamePattern1) 952**] regarding possible tracheobronchial plasty\\n\\nwith mesh. Tracheal stents d/c [**2119-4-19**] in anticipation of\\nsurgery.\\nIn terms of symptoms, she describes many years of intermittent\\nchest pain that she describes as left sided and occurring at any\\n\\ntime. Currently, she notices it about three times a week, and\\nstates that it seems to resolve after three nitroglycerin.\\nShe currently is dependent on oxygen and wears 1.5-2 liters\\naround the clock. She has frequent coughing and brings up \"dark\\nsputum\".\\n\\n\\n\\nPast Medical History:\\nCOPD flare [**6-7**] s/p intubation, s/p distal tracheal to Left Main\\nStem stents placed [**2118-6-9**]. Stents d/c\\'d [**2119-4-19**], CAD w/ atypical\\nangina (LAD 30%, RCA 30%, EF 63%), ^chol, hypothyroidism, htn,\\nhiatal hernia, lacunar CVA, s/p ped struck -> head injury & rib\\nfx, depression\\nPMH:\\nCOPD, s/p admit [**6-7**] for exacerbation requiring intubation\\ntracheobronchomalacia, s/p bronchial stenting\\nLarge hiatal hernia\\nLacunar CVA\\nHypothyroidism by records in CCC, although patient denies and is\\n\\nnot taking any medication\\nDepression\\nMVA, s/p head injury approximately 10 years ago\\nHypertension\\nHysterectomy\\n\\n\\nSocial History:\\nSocial History: The patient is married and worked as a clinical\\npsychologist. Her husband is a pediatric neurologist at\\n[**Hospital3 **]. They have several children, one of which is\\n\\na nurse.\\n\\n\\nFamily History:\\nFamily History: (+) FHx CAD; Father with an MI in his 40\\'s, died\\n\\nof a CVA at age 59\\n\\n\\nPhysical Exam:\\nAdmit H+P\\nGeneral-lovely 81 yr old feamle in NAD.\\nNeuro- intermittently anxious, MAE, PERRLA, L eye ptosis,\\nsymetrical smile, gossly intact.\\nHEENT-PERRLA, sclera anicteric, pharynx- no exud or erythema\\nResp-clear upper, diffuse ronchi, intermit exp wheezes\\nCor- RRR, No M, R, G\\nAbd- soft, NT, ND, no masses. Slight protrusion at area of\\nhiatal hernia\\nExt- no edema or clubbing\\n\\nBrief Hospital Course:\\n82 y/o female admitted [**2119-5-4**] for consideration of\\ntracheoplasty.\\nBronchoscopy done [**5-4**] confirming severe TBM. Underwent\\ntracheoplasty [**5-5**], complicated by resp failure d/t mucous\\nplugging, hypoxia requiring re-intubation resulting in prolonged\\nICU and hospital course. Also developed right upper extrem DVT\\nfrom mid line.\\n\\nPain- Epidural accidently d/c\\'d POD#1, pt briefly used dilaudid\\nPCA intermittently w/ fair pain control. Pt required\\nre-intubation for resp failure d/t secretions and PCA d/c at\\nthat time. Propofol for sedation while intubated. Sedation d/c\\'d\\n[**5-12**] for weaning trial w/ ETT- failed trial. Trach [**5-13**]-weaning\\nefforts as below. Minimal c/o pain since [**5-13**]. Presently pain\\nfree.\\n\\nNeuro- Initially intact- post op aggitation, inhibiting weaning\\nefforts [**5-16**]. Psych eval [**5-18**]-Started on zyprexa and ativan w/\\nimprovement in anxiety. Presently A+Ox3- cooperative and lovely.\\n\\nResp- Extubated POD#2 then required re-intub  [**5-7**] for hypoxia\\nd/t poor cough and mucous plugging. SIMV/PS alt w/CMV at night\\nx4-5d, with CPAP attempts during day.\\nBronchoscopy qd [**Date range (1) 1813**] for secretion management. Bronch [**5-9**]\\nrevealed swollen epiglottis, bronch [**5-10**] - good leak w/ ETT cuff\\ndeflated. Bronch [**5-13**] for eval/trach placement. Last bronch [**5-19**]\\nw/ min secretions present, sputum sent.\\n[**5-13**] perc trach done(#8 Portex- cuffed low pressure maintained to\\npreserve tracheoplasty site). [**5-13**] CPAP15/peep5 initiated post\\ntrach placement. Weaning ongoing.  [**Date range (1) 1814**]- Aggressive weaning\\nw/ increasing episodes of CPAP, progressing to Trach Mask.\\n[**2033-5-20**]-Trach mask overnight w/ no episodes of SOB, or\\nhemodynamic instability. Trach changed to #6 portex- capped and\\n[**Last Name (un) 1815**] well x48hrs on 2LNP. productive cough. Aggressive PT as\\nwell w/ OOB > chair [**Hospital1 **]-tid to total 4-6hr qd. Ambulation\\n~100-120 ft [**5-22**] w/ PT assist.\\n\\nID- Vancomycin started post-op for graft prophylaxis. Fever\\nspike [**2119-5-8**] w/ BAl & sputum sent> + MRSA. Vanco cont to [**4-7**]\\nweeks post trachealplasty. Fever low grade [**5-12**], [**5-15**]> cultured-\\nno new results. [**5-19**]- WBC 20.8 .\\n\\nCardiac-Hypertension controlled w/ hydralazine IV, then d/c and\\ncont controlled. HR 65-75 NSR. Avoiding B Blockers. Lasix 20mg\\nIV qd.\\n[**5-15**]- RUE redness and swelling at site of midline, RUE DVT by\\nultrasound, midline d/c; heparin gtt started and therapeutic\\nrange monitored. [**5-22**]  changed to Lovenox sq [**Hospital1 **]. Coags in good\\ncontrol [**5-23**] (48.2/13.8/1.2)\\nAccess- R midline placed [**2119-5-9**] for access- clotted [**2119-5-15**] and\\nd/c\\'d.  RUE redness and swelling and DVT via ultrasound. [**5-15**]- L\\nbrachial PICC line placed- TPN resumed.\\n\\nGI-Large hiatal hernia- unable to place enteral feeding tube at\\nbedside or underfluoro. Re-attempt [**5-17**] by EGD doboff tube\\nplaced distal esophagus, dislodged in 12hours and removed.\\n\\nNutrition- PPN/TPN initiated [**2119-5-8**]- [**2119-5-25**]. PICC placed\\n[**2119-5-15**]. Speech and Swallow eval [**5-22**]- rec change trach form #8\\nto #6 Portex to allow improved epiglotis and oropharyngeal\\nmovement to assist w/ swallowing. Then re-eval.  Trach changed\\n[**5-23**] to #6 cuffless portex trach. Passed repeat swallow eval and\\n[**Last Name (un) 1815**] diet of regular solids w/ thin liquids- CHIN TUCK to\\nswallow thin liquids. Give meds whole w/ apple sauce. WOULD\\nRECOMMEND repeat video swallow eval in [**8-17**] days to possibly\\neliminate chin tuck- see page 3 referral.\\n\\nEndo- Hypothyroid, maintained on levoxyl.\\n\\nMuscu/Skel- OOB> chair 4-6hours/day, PT consulting.\\n\\n\\nMedications on Admission:\\nadvair 250/50\", atrovent, imdur 60\\', lasix 40\\', lexapro 20\\',\\nlipitor 10\\', prilosec 20\\', mucinex 600\", synthroid 75\\', detrol\\nLA 4\\', ambien 5\\', trazadone 75\\', melatonin prn\\n\\nDischarge Medications:\\n1. Albuterol Sulfate 0.083 % Solution Sig: One (1)  Inhalation\\nQ6H (every 6 hours) as needed for wheezing.\\n2. Ipratropium Bromide 0.02 % Solution Sig: One (1)  Inhalation\\nQ6H (every 6 hours) as needed for wheezing.\\n3. Fluticasone-Salmeterol 250-50 mcg/Dose Disk with Device Sig:\\nOne (1) Disk with Device Inhalation [**Hospital1 **] (2 times a day).\\n4. Albuterol 90 mcg/Actuation Aerosol Sig: 1-2 Puffs Inhalation\\nQ6H (every 6 hours) as needed.\\n5. Ipratropium Bromide 18 mcg/Actuation Aerosol Sig: Two (2)\\nPuff Inhalation QID (4 times a day).\\n6. Acetaminophen 325 mg Tablet Sig: 1-2 Tablets PO Q4-6H (every\\n4 to 6 hours) as needed.\\n7. Sodium Chloride 0.65 % Aerosol, Spray Sig: [**2-5**] Sprays Nasal\\nQID (4 times a day) as needed.\\n8. Camphor-Menthol 0.5-0.5 % Lotion Sig: One (1) Appl Topical\\nTID (3 times a day) as needed.\\n9. Enoxaparin Sodium 60 mg/0.6mL Syringe Sig: One (1)\\nSubcutaneous Q12H (every 12 hours).\\n10. Trazodone HCl 50 mg Tablet Sig: 1.5 Tablets PO HS (at\\nbedtime) as needed.\\n11. Escitalopram Oxalate 10 mg Tablet Sig: Two (2) Tablet PO\\nDAILY (Daily).\\n12. Nystatin 100,000 unit/g Cream Sig: One (1) Appl Topical  [**Hospital1 **]\\n(2 times a day).\\n13. Pantoprazole Sodium 40 mg Tablet, Delayed Release (E.C.)\\nSig: One (1) Tablet, Delayed Release (E.C.) PO Q24H (every 24\\nhours).\\n14. Tolterodine Tartrate 2 mg Tablet Sig: One (1) Tablet PO BID\\n(2 times a day).\\n15. Levothyroxine Sodium 75 mcg Tablet Sig: One (1) Tablet PO\\nDAILY (Daily).\\n16. Heparin Lock Flush (Porcine) 100 unit/mL Syringe Sig: One\\n(1) ML Intravenous  DAILY (Daily) as needed.\\n\\n\\nDischarge Disposition:\\nExtended Care\\n\\nFacility:\\n[**Hospital3 7**] & Rehab Center - [**Hospital1 8**]\\n\\nDischarge Diagnosis:\\nCOPD, Coronary Artery Disease/atypical angina (LAD 30%, RCA 30%,\\nEF 63%), hypercholesterolemia, hypothyroidism, Hypertension,\\nhiatal hernia, Cerebral Vascular Accident,s/p Motor Vehicle\\nColision-> head injury & rib fracture.\\nTBM- s/p tracheoplasty.\\n\\n\\nDischarge Condition:\\ngood\\n\\nDischarge Instructions:\\nplease update Dr.[**Name (NI) 1816**] [**Telephone/Fax (1) 170**] office for:  fever,\\nshortness of breath, chest pain , productive cough or if you\\nhave any questions or concerns.\\n\\n\\nCompleted by:[**2119-5-25**]',\n",
       " ['Admission',\n",
       "  'Date:',\n",
       "  '[**2151-7-16**]',\n",
       "  'Discharge',\n",
       "  'Date:',\n",
       "  '[**2151-8-4**]',\n",
       "  'Service:',\n",
       "  'ADDENDUM:',\n",
       "  'RADIOLOGIC',\n",
       "  'STUDIES:',\n",
       "  'Radiologic',\n",
       "  'studies',\n",
       "  'also',\n",
       "  'included',\n",
       "  'a',\n",
       "  'chest',\n",
       "  'CT,',\n",
       "  'which',\n",
       "  'confirmed',\n",
       "  'cavitary',\n",
       "  'lesions',\n",
       "  'in',\n",
       "  'the',\n",
       "  'left',\n",
       "  'lung',\n",
       "  'apex',\n",
       "  'consistent',\n",
       "  'with',\n",
       "  'infectious',\n",
       "  'process/tuberculosis.',\n",
       "  'This',\n",
       "  'also',\n",
       "  'moderate-sized',\n",
       "  'left',\n",
       "  'pleural',\n",
       "  'effusion.',\n",
       "  'HEAD',\n",
       "  'CT:',\n",
       "  'Head',\n",
       "  'CT',\n",
       "  'showed',\n",
       "  'no',\n",
       "  'intracranial',\n",
       "  'hemorrhage',\n",
       "  'or',\n",
       "  'mass',\n",
       "  'effect,',\n",
       "  'but',\n",
       "  'old',\n",
       "  'infarction',\n",
       "  'consistent',\n",
       "  'with',\n",
       "  'past',\n",
       "  'medical',\n",
       "  'history.',\n",
       "  'ABDOMINAL',\n",
       "  'CT:',\n",
       "  'Abdominal',\n",
       "  'CT',\n",
       "  'showed',\n",
       "  'lesions',\n",
       "  'of',\n",
       "  'T10',\n",
       "  'and',\n",
       "  'sacrum',\n",
       "  'most',\n",
       "  'likely',\n",
       "  'secondary',\n",
       "  'to',\n",
       "  'osteoporosis.',\n",
       "  'These',\n",
       "  'can',\n",
       "  'be',\n",
       "  'followed',\n",
       "  'by',\n",
       "  'repeat',\n",
       "  'imaging',\n",
       "  'as',\n",
       "  'an',\n",
       "  'outpatient.',\n",
       "  '[**First',\n",
       "  'Name8',\n",
       "  '(NamePattern2)',\n",
       "  '**]',\n",
       "  '[**First',\n",
       "  'Name4',\n",
       "  '(NamePattern1)',\n",
       "  '1775**]',\n",
       "  '[**Last',\n",
       "  'Name',\n",
       "  '(NamePattern1)',\n",
       "  '**],',\n",
       "  'M.D.',\n",
       "  '[**MD',\n",
       "  'Number(1)',\n",
       "  '1776**]',\n",
       "  'Dictated',\n",
       "  'By:[**Hospital',\n",
       "  '1807**]',\n",
       "  'MEDQUIST36',\n",
       "  'D:',\n",
       "  '[**2151-8-5**]',\n",
       "  '12:11',\n",
       "  'T:',\n",
       "  '[**2151-8-5**]',\n",
       "  '12:21',\n",
       "  'JOB#:',\n",
       "  '[**Job',\n",
       "  'Number',\n",
       "  '1808**]'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk.head()['TEXT'][2], chunk.head()['TEXT'][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('acros.pickle', 'rb') as handle:\n",
    "    acro_dct = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a [u'assessment', u'a']\n",
      "abd [u'abdominal', u'abdomen', u'army battle dressing']\n",
      "abg [u'arterial blood gas']\n",
      "aicd [u'automated implantable cardioverter-defibrillator']\n",
      "air [u'anterior interval release', u'acute inpatient rehabilitation', u'air']\n",
      "all [u'allergies', u'acute lymphoblastic leukemia', u'all']\n",
      "alt [u'alanine transaminase']\n",
      "and [u'allowing natural death', u'and']\n",
      "ap [u'apical', u'before a meal', u'action potential', u'area postrema', u'alkaline phosphatase', u'angina pectoris', u'anteroposterior']\n",
      "as [u'atherosclerosis', u'aortic stenosis', u'ankylosing spondylitis', u'as']\n",
      "asa [u'american society of anesthesiologists', u'acetylsalicylic acid']\n",
      "at [u'antithrombin', u'angiotensin', u'at']\n",
      "b/l [u'bilateral']\n",
      "bal [u'blood alcohol level', u'british anti-lewisite', u'bronchoalveolar lavage']\n",
      "be [u'barium enema', u'base excess', u'be']\n",
      "bid [u'twice a day']\n",
      "bm [u'bone marrow', u'bowel movement', u'breast milk']\n",
      "bp [u'british pharmacopoeia', u'blood pressure']\n",
      "bs [u'breath', u'bowel sounds', u'body secretions', u'breath sound', u'blood sugar']\n",
      "c [u'means with', u'with', u'cytosine', u'cervical vertebrae']\n",
      "c-spine [u'cervical spine']\n",
      "c/o [u'complains of']\n",
      "c1 [u'atlas \\xe2\\u20ac\\u201c first cervical vertebra of the spine']\n",
      "cabg [u'coronary artery bypass graft']\n",
      "cad [u'coronary artery disease']\n",
      "cath [u'catheter']\n",
      "chf [u'continuous hemofiltration', u'congestive heart failure']\n",
      "cm [u'cardiomyopathy', u'caucasian male', u'chirurgiae magister master of surgery']\n",
      "cmo [u'comfort measures only']\n",
      "copd [u'chronic obstructive pulmonary disease']\n",
      "cpap [u'continuous positive airway pressure']\n",
      "ct [u'computed tomography', u'computed axial tomography', u'computed tomography', u'cervicothoracic', u'cat']\n",
      "cta [u'clear to auscultation', u'computed tomography angiography']\n",
      "cv [u'cardiovascular']\n",
      "cva [u'cerebrovascular accident', u'costovertebral angle']\n",
      "cxr [u'chest x-ray']\n",
      "d [u'day']\n",
      "d/c [u'discharge', u'discontinue']\n",
      "d/t [u'due to']\n",
      "dnr [u'do not resuscitate']\n",
      "do [u'doctor of osteopathic medicine', u'do']\n",
      "dvt [u'deep vein thrombosis']\n",
      "ec [u'enteric coating']\n",
      "echo [u'enteric cytopathic human orphan virus']\n",
      "ed [u'effective dose', u'emotional distress', u'ectodermal dysplasia', u'erectile dysfunction', u'emergency department']\n",
      "ef [u'ejection fraction']\n",
      "egd [u'esophagogastroduodenoscopy']\n",
      "ekg [u'electrocardiogram']\n",
      "ems [u'emergency medical service']\n",
      "er [u'emergency room']\n",
      "ett [u'endotracheal tube']\n",
      "exam [u'examination']\n",
      "ext [u'extremities']\n",
      "fast [u'focused assessment with sonography for trauma', u'fast']\n",
      "fev1 [u'forced expiratory volume in 1 second']\n",
      "fhx [u'family history']\n",
      "from [u'full range of motion', u'from']\n",
      "fvc [u'forced vital capacity']\n",
      "fx [u'fracture', u'function']\n",
      "g [u'guanosine', u'gravidity']\n",
      "gave [u'gastric antral vascular ectasia', u'gave']\n",
      "gi [u'gastrointestinal', u'gastrointestinal tract']\n",
      "gtt [u'glucose tolerance test  gestational trophoblastic tumor']\n",
      "gu [u'genitourinary', u'gastric ulcer']\n",
      "h [u'hr', u'hours', u'its receptors', u'histamine', u'hemagglutinin']\n",
      "h/o [u'history of']\n",
      "had [u'hiv-associated dementia', u'had']\n",
      "hcl [u'hairy cell leukemia']\n",
      "he [u'hepatic encephalopathy', u'he']\n",
      "heent [u'head eyes ears nose throat']\n",
      "help [u'hemolysis elevated liver enzymes low platelets', u'help']\n",
      "hr [u'heart rate', u'hours', u'hour']\n",
      "hs [u'at bedtime', u'hours of sleep']\n",
      "htn [u'hypertension']\n",
      "icu [u'intensive care unit']\n",
      "if [u'immunofluorescence', u'if']\n",
      "ir [u'insulin resistance', u'interventional radiology']\n",
      "is [u'incentive spirometry', u'is']\n",
      "it [u'immature teratoma', u'intrathecal', u'it']\n",
      "iv [u'intravenous']\n",
      "ivdu [u'intravenous drug user']\n",
      "jvp [u'jugular venous pressure']\n",
      "l [u'lumbar vertebrae', u'leukocytes']\n",
      "la [u'left atrium', u'local anesthetic', u'lymphadenopathy']\n",
      "lad [u'leukocyte adhesion deficiency', u'lymphadenopathy', u'left axis deviation', u'left anterior descending']\n",
      "le [u'lupus erythematosus', u'lower extremity']\n",
      "lvh [u'left ventricular hypertrophy']\n",
      "m [u'murmur']\n",
      "mae [u'moves all extremities']\n",
      "mdi [u'metered dose inhaler']\n",
      "mesh [u'medical subject headings']\n",
      "mg [u'magnesium', u'myasthenia gravis']\n",
      "mi [u'myocardial infarction']\n",
      "micu [u'medical intensive care unit', u'mobile intensive care unit']\n",
      "mmm [u'myelofibrosis with myeloid metaplasia', u'moist mucus membranes']\n",
      "mr [u'medical representative', u'mitral regurgitation', u'modified release', u'mental retardation', u'menstrual regulation']\n",
      "mri [u'magnetic resonance imaging']\n",
      "mrsa [u'methicillin-resistant staphylococcus aureus']\n",
      "mva [u'motor vehicle accident']\n",
      "nad [u'no apparent distress', u'no abnormality detected']\n",
      "nd [u'not done']\n",
      "ng [u'nasogastric']\n",
      "no [u'nitric oxide', u'number', u'no']\n",
      "nrb [u'non-rebreather mask']\n",
      "ns [u'not significant', u'normal saline']\n",
      "nsr [u'normal sinus rhythm']\n",
      "nt [u'not tested', u'nuchal translucency']\n",
      "o2 [u'oxygen']\n",
      "on [u'every night  generally written in lowercase', u'on']\n",
      "oob [u'out of bed']\n",
      "op [u'outpatient department', u'osteoporosis']\n",
      "or [u'operating room', u'odds ratio', u'or']\n",
      "p [u'after', u'post', u'pulse', u'parturition', u'phosphorus']\n",
      "pad [u'peripheral artery disease', u'passively acquired anti-d', u'peripheral airspace disease', u'postadmission day']\n",
      "pca [u'patient-controlled analgesia', u'prostate cancer']\n",
      "pcp [u'pneumocystis carinii pneumonia', u'primary care physician']\n",
      "pcwp [u'pulmonary capillary wedge pressure']\n",
      "peep [u'positive end expiratory pressure']\n",
      "peg [u'percutaneous endoscopic gastrostomy']\n",
      "perrla [u'pupils equal round reactive to light and accommodation']\n",
      "ph [u'past history', u'pulmonary hypertension']\n",
      "picc [u'peripherally inserted central catheter']\n",
      "plt [u'platelets']\n",
      "pm [u'post meridiem']\n",
      "pmh [u'past medical history', u'perimesencephalic subarachnoid hemorrhage']\n",
      "pnd [u'postnasal drip', u'paroxysmal nocturnal dyspnea']\n",
      "po [u'by mouth']\n",
      "post [u'posterior', u'post']\n",
      "ppi [u'proton pump inhibitor']\n",
      "prn [u'as necessary', u'as needed']\n",
      "pt [u'physical therapy', u'prothrombin time', u'patient']\n",
      "q6h [u'once every 6 hours']\n",
      "qd [u'each day']\n",
      "qhs [u'every bedtime']\n",
      "qid [u'four times each day']\n",
      "r [u'respiration']\n",
      "ra [u'rheumatoid arthritis', u'right atrium', u'refractory anemia', u'room air']\n",
      "rca [u'right coronary artery']\n",
      "rrr [u'regular rate and rhythm']\n",
      "rue [u'right upper extremity']\n",
      "rv [u'right ventricle', u'residual volume']\n",
      "s/p [u'status post']\n",
      "s1 [u'first heart sound']\n",
      "s2 [u'second heart sound']\n",
      "sad [u'subacromial decompression', u'seasonal affective disorder']\n",
      "sbp [u'systolic blood pressure', u'spontaneous bacterial peritonitis']\n",
      "see [u'syphilis elimination effort', u'see']\n",
      "sit [u'stress inoculation training', u'sit']\n",
      "so [u'salpingo-oophoritis', u'so']\n",
      "sob [u'shortness of breath']\n",
      "sq [u'subcutaneous']\n",
      "stop [u'surgical termination of pregnancy', u'stop']\n",
      "subq [u'subcutaneous']\n",
      "t [u'thoracic vertebrae']\n",
      "tid [u'three times a day']\n",
      "tpn [u'total parenteral nutrition']\n",
      "tsh [u'thyroid stimulating hormone', u'thoughts of self-harm']\n",
      "tte [u'transthoracic echocardiogram']\n",
      "ue [u'upper extremity']\n",
      "vs [u'versus', u'vital signs']\n",
      "was [u'wiskott\\xe2\\u20ac\\u201caldrich syndrome', u'was']\n",
      "wbc [u'white blood cell count', u'white blood cell']\n",
      "x [u'except']\n",
      "y/o [u'years old']\n",
      "yo [u'years old']\n",
      "yr [u'years', u'year']\n"
     ]
    }
   ],
   "source": [
    "o=set()\n",
    "for line in chunk.head()['TEXT'][:30]:\n",
    "    l= (re.sub(r'\\b\\d+\\b', ' number_phrase ',\n",
    "        re.sub(r'[\\[]\\*\\*.*?\\*\\*[\\]]', ' redacted_phrase ', line).translate(None, note_punc).lower()).split())\n",
    "    for i in l:\n",
    "        if i in acro_dct:\n",
    "            o.add(i)\n",
    "for i in sorted(o):\n",
    "    print i, acro_dct[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(386, 2279)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len([j  for j in acro_dct.values() if len(j)>1]), len(acro_dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/rishubj/text/MIMIC-data/smallestNOTES.csv') as f:\n",
    "#     l = [x.strip() for x in f.readlines()] #tf.compat.as_str(f.read()).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'174,22532,167853,2151-08-04,,,\"Discharge summary\",\"Report\",,,\"Admission Date:  [**2151-7-16**]       Discharge Date:  [**2151-8-4**]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# l[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "def collect_data(vocabulary_size, url):\n",
    "    filename = url\n",
    "    vocabulary = read_data(filename)\n",
    "    print(vocabulary[:7])\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(vocabulary,\n",
    "                                                                vocabulary_size)\n",
    "    del vocabulary  # Hint to reduce memory.\n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0677e2ced206>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mvocabulary_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_dictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-58a645b3efa5>\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(vocabulary_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://mattmahoney.net/dc/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8.zip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31344016\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-58a645b3efa5>\u001b[0m in \u001b[0;36mmaybe_download\u001b[0;34m(filename, url, expected_bytes)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mstatinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstatinfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mexpected_bytes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data, context)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0murlcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_urlopener\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self, url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mreporthook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblocknum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/socket.pyc\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0;31m# fragmentation issues on many platforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "vocabulary_size = 10000\n",
    "data, count, dictionary, reverse_dictionary = collect_data(vocabulary_size=vocabulary_size)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 300  # Dimension of the embedding vector.\n",
    "skip_window = 2       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "            tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "    # Construct the variables for the softmax\n",
    "    weights = tf.Variable(\n",
    "            tf.truncated_normal([embedding_size, vocabulary_size],\n",
    "                                                    stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    hidden_out = tf.transpose(tf.matmul(tf.transpose(weights), tf.transpose(embed))) + biases\n",
    "\n",
    "    # convert train_context to a one-hot format\n",
    "    train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, labels=train_one_hot))\n",
    "\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "\n",
    "    # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    valid_embeddings = tf.nn.embedding_lookup(\n",
    "            normalized_embeddings, valid_dataset)\n",
    "    similarity = tf.matmul(\n",
    "            valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "def run(graph, num_steps):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(data,\n",
    "                    batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "            # We perform one update step by evaluating the optimizer op (including it\n",
    "            # in the list of returned values for session.run()\n",
    "            _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "            average_loss += loss_val\n",
    "\n",
    "            if step % 2000 == 0:\n",
    "                if step > 0:\n",
    "                    average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "                print('Average loss at step ', step, ': ', average_loss)\n",
    "                average_loss = 0\n",
    "\n",
    "            # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "            if step % 10000 == 0:\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                    top_k = 8  # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                    log_str = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = reverse_dictionary[nearest[k]]\n",
    "                        log_str = '%s %s,' % (log_str, close_word)\n",
    "                    print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# num_steps = 100\n",
    "# softmax_start_time = dt.datetime.now()\n",
    "# run(graph, num_steps=num_steps)\n",
    "# softmax_end_time = dt.datetime.now()\n",
    "# print(\"Softmax method took {} minutes to run 100 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    nce_loss = tf.reduce_mean(\n",
    "        tf.nn.nce_loss(weights=nce_weights,\n",
    "                       biases=nce_biases,\n",
    "                       labels=train_context,\n",
    "                       inputs=embed,\n",
    "                       num_sampled=num_sampled,\n",
    "                       num_classes=vocabulary_size))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(nce_loss)\n",
    "\n",
    "    # Add variable initializer.\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "num_steps = 50000\n",
    "nce_start_time = dt.datetime.now()\n",
    "run(graph, num_steps)\n",
    "nce_end_time = dt.datetime.now()\n",
    "print(\"NCE method took {} minutes to run 100 iterations\".format((nce_end_time-nce_start_time).total_seconds()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
